---
title: "Five Key Realities About AI in 2025"
date: 2025-07-22
layout: summary.njk
tags: [ai, strategy]
summary: |
  Will Douglas Heaven outlines five major themes shaping the AI landscape in 2025, from rapid generative AI advances and energy demands to unresolved mysteries about model behaviour and the fuzzy meaning of AGI. The piece blends caution with curiosity, encouraging both amazement and scepticism.
---

## Source and link
- Authors: Will Douglas Heaven
- Venue: MIT Technology Review
- Link: https://www.technologyreview.com/2025/07/22/1120556/five-things-to-know-ai/

## What the paper claims
The author argues that AI in 2025 is marked by extraordinary capability gains, persistent and misunderstood limitations, increasing environmental impact, and ongoing conceptual confusion around terms like “AGI.” These trends demand a balanced approach that recognises both the promise and the pitfalls of the technology.

## Methods or approach
The piece distils five observations from a talk at SXSW London, drawing on industry developments, practical examples, and personal reflections. It uses accessible explanations and analogies to make technical and conceptual points understandable for a general audience.

## Key findings
- Generative AI has become so effective across media types that humans often cannot distinguish its outputs from human-made content.
- Hallucination is intrinsic to generative models and unlikely to disappear, as they are designed to fabricate plausible outputs.
- AI’s energy consumption is shifting from model training to widespread daily use, prompting major infrastructure expansion.
- Despite widespread deployment, the inner workings of large language models remain poorly understood.
- AGI remains an ill-defined and contested concept, often used to mean “better AI” without clear criteria.

## Limitations and caveats
- Observations are drawn from the author’s perspective and are not based on systematic empirical study.
- The focus is on high-level trends rather than detailed technical or policy analysis.

## Implications for practitioners
- Avoid assuming future AI models will naturally resolve issues like hallucination; design with these traits in mind.
- Factor in operational energy demands when planning AI deployments, not just training costs.
- Treat AGI claims with caution and demand precise definitions when used in strategic discussions.

## Notable passages quoted
> "Hallucination is a feature, not a bug." — Point 2

> "It’s incredible to think that never before has a mass-market technology used by billions of people been so little understood." — Point 4
